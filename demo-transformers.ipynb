{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Self-Attention and the Transformer\n",
    "CS-GY 9223 Deep Learning, Fall 2020\n",
    "\n",
    "## Some helpful visual aids:\n",
    "\n",
    "Transformer: http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "Attention: https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as f\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Self-attention\n",
    "\n",
    "Consider a set of $t$ inputs $\\{\\mathbf{x}_{i}\\}^{t}_{i=1} \\in \\mathbb{R}^{n}.$\n",
    "\n",
    "These $\\mathbf{x}$'s can form a matrix with $n$ rows and $t$ columns:\n",
    "\n",
    "$$\\mathbf{X} = \\begin{bmatrix} | & | &  & |\\\\\n",
    "\\mathbf{x}_{1}&\\mathbf{x}_{2}&\\cdots&\\mathbf{x}_{t}\\\\\n",
    "| & | &  & | \\end{bmatrix} \\in \\mathbb{R}^{n\\times t}.$$\n",
    "\n",
    "We then consider a hidden representation which is a linear combination of our column vectors $\\mathbf{x}_{i}$ : \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{h} &= \\alpha_{1}\\mathbf{x}_{1} + \\alpha_{2}\\mathbf{x}_{2} + \\cdots + \\alpha_{t}\\mathbf{x}_{t},\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "i.e.,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{h} &=\n",
    "\\begin{bmatrix} | & | &  & |\\\\\n",
    "\\mathbf{x}_{1}&\\mathbf{x}_{2}&\\cdots&\\mathbf{x}_{t}\\\\\n",
    "| & | &  & | \\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\alpha_{1}\\\\\n",
    "\\alpha_{2}\\\\\n",
    "\\vdots\\\\\n",
    "\\alpha_{t}\n",
    "\\end{bmatrix} = \\mathbf{X}\\mathbf{a} \\in \\mathbb{R}^{n}\n",
    "\\end{aligned}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Where do these coefficients come from? In a standard Transformer model, they are scores found by applying the softmax function:\n",
    "$\\mathbf{a} = \\text{softmax}(\\mathbf{X}^{\\top}\\mathbf{x})\\in\\mathbb{R}^{t}.$ [This](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/) is an excellent discussion of how softmax works if you need to refresh your memory.\n",
    "\n",
    "$\\mathbf{a}$ encodes the value of the dot product of input vector $\\mathbf{x}_{t}$ with every other vector in the set $\\mathbf{X}$. \n",
    "<!-- Every element is the scalar product of the whole set $\\mathbf{x}$ against a given $\\mathbf{x}$. -->\n",
    "\n",
    "<!-- Note: $\\beta$ is a parameter of the soft $\\arg\\max$ (softmax); in energy terms, the inverse of the temperature - the exponential of the argument divided by summation of all exponentials).  -->\n",
    "\n",
    "### \"Soft\" vs \"hard\" attention\n",
    "\n",
    "We can make a choice between using \"hard\" attention via $\\arg\\max$ and \"soft\" attention via $\\mathrm{softmax}$. In hard attention, $\\mathbf{a}$ is a one-hot vector, and multiplication by $\\mathbf{X}$ is a selection of a single column, choosing only one element of the set $\\mathbf{X}$ with maximum similarity score. In this case we have that the $L_0$ \"norm\" $\\lvert\\lvert\\mathbf{a}\\rvert\\rvert_{0} = 1.$ In soft attention, $\\mathbf{a}$ is a distribution which assigns a non-zero probability to every element in the set $\\mathbf{X}.$ Then we have that the $L_1$ norm $\\lvert\\lvert\\mathbf{a}\\rvert\\rvert_{1} = 1.$ If you are wondering whether there is a function which assigns a non-zero probability to only some of the elements, you are correct, and it is called [sparsemax](https://arxiv.org/abs/1602.02068).\n",
    "\n",
    "A set of $\\mathbf{x}$'s implies a set of $\\mathbf{a}$ score vectors, which can be stacked into a matrix $\\mathbf{A} \\in \\mathbb{R}^{t\\times t}$ (since $\\mathbf{a}$ has size $t$ for the $t$ rows in $\\mathbf{x}^{\\top}$). For the set of $\\mathbf{a}$'s we also have a set of $\\mathbf{h}$'s: $\\mathbf{H} \\in \\mathbb{R}^{n\\times t},$ so we can write finally\n",
    "\n",
    "$$\\mathbf{H} = \\mathbf{X}\\mathbf{A} \\in\\mathbb{R}.$$\n",
    "\n",
    "$\\mathbf{H}$ is a linear combination of the elements of $\\mathbf{X}$ using the factors in the columns of $\\mathbf{A}$.\n",
    "\n",
    "Overall, what we are doing is to mix the components of the set of $\\mathbf{x}$'s by using these coefficients which are computed using the soft argmax, where each component has a score of cosine similarity (dot product) of a given $\\mathbf{x}$ against the set $\\mathbf{X}.$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Key-value store\n",
    "\n",
    "Conceptually, we are checking how aligned is the query against all the values in the dataset (compute how matching the dataset values are with respect to your query). We can retrieve the single maximum matching element with $\\arg\\max$ or use soft $\\arg\\max$ to return a distribution which has a score for every element, in which case we can retrieve things with an ordering of similarity.\n",
    "\n",
    "Queries, keys, and values are rotations of input $\\mathbf{x}$: \n",
    "$$\\mathbf{q} = W_q \\mathbf{x}$$ \n",
    "$$\\mathbf{k} = W_k \\mathbf{x}$$ \n",
    "$$\\mathbf{v} = W_v \\mathbf{x}$$\n",
    "These rotations $W_q, W_k, W_v$ are training parameters.\n",
    "\n",
    "Attention is *completely based on affine orientation*: the only nonlinear operation is introduced by the softmax. $\\mathbf{q}$ and $\\mathbf{k}$ must have the same dimension; $\\mathbf{v}$ is the returned value/content associated with a given key, which can have any size, though in practice it is usually taken to be the same size as $\\mathbf{q}$ and $\\mathbf{k}$.\n",
    "\n",
    "Given that we have a set of $\\mathbf{x}$'s, we'll have a set of queries, keys, values, and we can make a matrix stacking them all up. This matrix has $t$ cols of row vectors of size $d$. In the attention operation, we check a query $\\mathbf{q}$ against all keys by applying $\\mathbf{K}^{\\top}\\mathbf{q}$. This returns $t$ scores which constitute a probability distribution over the space of possible matching sequences."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transformer Model\n",
    "\n",
    "## Multi-head attention module"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "nn_Softargmax = nn.Softmax  # a more correct/descriptive name"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# multiple heads: allows for multiple properties per query\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, p, d_input=None):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        if d_input is None:\n",
    "            d_xq = d_xk = d_xv = d_model\n",
    "        else:\n",
    "            d_xq, d_xk, d_xv = d_input\n",
    "            \n",
    "        # Make sure that the embedding dimension of model is a multiple of number of heads\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.d_k = d_model // self.num_heads\n",
    "\n",
    "        # matrices allowing to rotate current input\n",
    "        # (These are still of dimension d_model. They will be split into number of heads)\n",
    "        self.W_q = nn.Linear(d_xq, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_xk, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_xv, d_model, bias=False)\n",
    "        \n",
    "        # Outputs of all sub-layers need to be of dimension d_model\n",
    "        self.W_h = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V):\n",
    "        batch_size = Q.size(0)\n",
    "        k_length = K.size(-2) \n",
    "        \n",
    "        # Scaling by d_k so that the soft(arg)max doesn't saturate\n",
    "        Q = Q / np.sqrt(self.d_k) # (bs, n_heads, q_length, dim_per_head)\n",
    "\n",
    "        # multiplication between one query and all keys\n",
    "        scores = torch.matmul(Q, K.transpose(2,3)) # (bs, n_heads, q_length, k_length)\n",
    "\n",
    "        # compute the mixing coefficients\n",
    "        A = nn_Softargmax(dim=-1)(scores) # (bs, n_heads, q_length, k_length)\n",
    "        \n",
    "        # get the weighted average of the values - multipy mixing coeff with V matrix\n",
    "        H = torch.matmul(A, V) # (bs, n_heads, q_length, dim_per_head)\n",
    "\n",
    "        return H, A\n",
    "\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Split the last dimension into (heads X depth)\n",
    "        Return after transpose to put in shape (batch_size X num_heads X seq_length X d_k)\n",
    "        \"\"\"\n",
    "        return x.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def group_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Combine the heads again to get (batch_size X seq_length X (num_heads times d_k))\n",
    "        \"\"\"\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "    \n",
    "\n",
    "    def forward(self, X_q, X_k, X_v):\n",
    "        batch_size, seq_length, dim = X_q.size()\n",
    "\n",
    "        # apply W transformation (learned rotation of x input), then split into num_heads \n",
    "        Q = self.split_heads(self.W_q(X_q), batch_size)  # (bs, n_heads, q_length, dim_per_head)\n",
    "        K = self.split_heads(self.W_k(X_k), batch_size)  # (bs, n_heads, k_length, dim_per_head)\n",
    "        V = self.split_heads(self.W_v(X_v), batch_size)  # (bs, n_heads, v_length, dim_per_head)\n",
    "        \n",
    "        # compute scaled dot product between one query against all keys\n",
    "        # i.e. calculate the attention weights for each of the heads\n",
    "        H_cat, A = self.scaled_dot_product_attention(Q, K, V)\n",
    "        \n",
    "        # Put all the heads back together by concat\n",
    "        H_cat = self.group_heads(H_cat, batch_size)  # (bs, q_length, dim)\n",
    "        \n",
    "        # Final linear layer  \n",
    "        H = self.W_h(H_cat)  # (bs, q_length, dim)\n",
    "        \n",
    "        return H, A"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check how the self-attention mechanism works:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8, p=0)\n",
    "def print_out(Q, K, V):\n",
    "    temp_out, temp_attn = temp_mha.scaled_dot_product_attention(Q, K, V)\n",
    "    print('Attention weights are:', temp_attn.squeeze())\n",
    "    print('Output is:', temp_out.squeeze())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To check our self attention does what we expect: if the query matches with one of the key values, it should have all the \"attention\" focused there, with the value returned being the value at that index."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_K = torch.tensor(\n",
    "    [[10, 0, 0],\n",
    "     [ 0,10, 0],\n",
    "     [ 0, 0,10],\n",
    "     [ 0, 0,10]]\n",
    ").float()[None, None]\n",
    "\n",
    "test_V = torch.tensor(\n",
    "    [[   1,0,0],\n",
    "     [  10,0,0],\n",
    "     [ 100,5,0],\n",
    "     [1000,6,0]]\n",
    ").float()[None, None]\n",
    "\n",
    "test_Q = torch.tensor(\n",
    "    [[0, 10, 0]]\n",
    ").float()[None, None]\n",
    "\n",
    "print_out(test_Q, test_K, test_V)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that it focuses on the second key and returns the second value. \n",
    "\n",
    "If we give a query that matches two keys exactly, it should return the averaged value of the two values for those two keys. "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_Q = torch.tensor([[0, 0, 10]]).float()  \n",
    "print_out(test_Q, test_K, test_V)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that it focuses equally on the third and fourth key and returns the average of their values.\n",
    "\n",
    "Now, passing all the queries at the same time:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_Q = torch.tensor(\n",
    "    [[0, 0, 10], [0, 10, 0], [10, 10, 0]]\n",
    ").float()[None,None]\n",
    "print_out(test_Q, test_K, test_V)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1D convolution with `kernel_size = 1`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is equivalent to an MLP with one hidden layer and ReLU activation applied to each and every element in the set."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# element-wise feedforward = 1d convolution with kernel size 1\n",
    "# linear layer maps a representation to some other representation (is a transformation)\n",
    "# convolution maps one set to another set - which is what we are actually doing here\n",
    "# apply same linear transform to every element in a sequence\n",
    "\n",
    "# conv hidden layer is applied to every component in the set - every element treated separately\n",
    "# if you apply same linear layer to every element in a sequence -> that's a convolution\n",
    "# in practice, implementations generally use a linear layer\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, d_model, hidden_dim, p):\n",
    "        super().__init__()\n",
    "        self.k1convL1 = nn.Linear(d_model,    hidden_dim)\n",
    "        self.k1convL2 = nn.Linear(hidden_dim, d_model)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.k1convL1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.k1convL2(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformer encoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Components of encoder block:\n",
    "# 1: self attention\n",
    "# 2: convolution - MLP applied to very element in the set\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, conv_hidden_dim, p=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads, p)\n",
    "        self.cnn = CNN(d_model, conv_hidden_dim, p)\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "        self.layernorm2 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Multi-head attention\n",
    "        attn_output, _ = self.mha(x, x, x)  # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        # Layer norm after adding the residual connection \n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        # Feed forward \n",
    "        cnn_output = self.cnn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        #Second layer norm after adding residual connection \n",
    "        out2 = self.layernorm2(out1 + cnn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Positional Embeddings\n",
    "\n",
    "See https://kazemnejad.com/blog/transformer_architecture_positional_encoding/ for illustration.\n",
    "\n",
    "The attention operation we have defined above is permutation equivariant. For input which is ordered, such as words in a sentence, we need to somehow account for the order of the words. In the classic Transformer, we add information about position not in the model itself, but by enhancing each value in the input with some information about its position. Since the Transformer architecture is equipped with residual connections, the positional information in the input is also able to propagate directly to further layers.\n",
    "\n",
    "Some criteria for a position-sensitive encoding function:\n",
    "- Should output a unique encoding for each time-step/word position in a sentence\n",
    "- Distance between any two time-steps should be consistent across sentences with different lengths\n",
    "- Model should generalize to longer sentences without any efforts; values should be bounded\n",
    "- Must be deterministic\n",
    "\n",
    "As such: let $t$ be the desired position in an input sentence, $p_{t} \\in \\mathbb{R}^{d}$ be its corresponding encoding, and $d$ be the encoding dimension. The positional embedding is a transformation of the word embedding:\n",
    "\n",
    "$$\\psi^{\\prime}(w_t) = \\psi(w_t)+p_t.$$\n",
    "\n",
    "Sinusoidal positional embeddings, for example, can be defined with:\n",
    "\n",
    "\\begin{aligned}\n",
    "E(p, 2i)    &= \\sin(p / 10000^{2i / d}) \\\\\n",
    "E(p, 2i+1) &= \\cos(p / 10000^{2i / d}),\n",
    "\\end{aligned}\n",
    "\n",
    "so that the positional embedding $p_t$ is a vector containing pairs of sines and cosines.\n",
    "\n",
    "<!-- - represents $p_{t+\\phi}$ as a linear function of $p_t$ for any fixed offset $\\phi$ - the sines and cosines implement a rotation transformation -->\n",
    "\n",
    "<!-- - position as the frequency of flip in value when incrementing, which varies depending on the bit position -> sinusoidal functions as the continuous version of alternating bits -->\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_sinusoidal_embeddings(nb_p, dim, E):\n",
    "    theta = np.array([\n",
    "        [p / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)]\n",
    "        for p in range(nb_p)\n",
    "    ])\n",
    "    E[:, 0::2] = torch.FloatTensor(np.sin(theta[:, 0::2]))\n",
    "    E[:, 1::2] = torch.FloatTensor(np.cos(theta[:, 1::2]))\n",
    "    E.detach_()\n",
    "    E.requires_grad = False\n",
    "    E = E.to(device)\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size, max_position_embeddings, p):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, d_model, padding_idx=1) # a simple lookup table that stores embeddings of a fixed dictionary and size\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, d_model)\n",
    "        create_sinusoidal_embeddings(\n",
    "            nb_p=max_position_embeddings,\n",
    "            dim=d_model,\n",
    "            E=self.position_embeddings.weight\n",
    "        )\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(d_model, eps=1e-12)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device) # (max_seq_length)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)                      # (b, max_seq_length)\n",
    "        \n",
    "        # Get word embeddings for each input id\n",
    "        word_embeddings = self.word_embeddings(input_ids) # (b, max_seq_length, dim)\n",
    "        \n",
    "        # Get position embeddings for each position id \n",
    "        position_embeddings = self.position_embeddings(position_ids) # (b, max_seq_length, dim)\n",
    "        \n",
    "        # Add them both \n",
    "        embeddings = word_embeddings + position_embeddings  # (b, max_seq_length, dim)\n",
    "        \n",
    "        # Layer norm \n",
    "        embeddings = self.LayerNorm(embeddings) # (b, max_seq_length, dim)\n",
    "        return embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Overall Encoder \n",
    "#### (Blocks of N Encoder Layers + Positional encoding + Input embedding)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_layers, \n",
    "                 d_model, \n",
    "                 num_heads, \n",
    "                 ff_hidden_dim, \n",
    "                 input_vocab_size,\n",
    "                 maximum_position_encoding, p=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # apply permutation-sensitive embeddings\n",
    "        self.embedding = Embeddings(d_model, \n",
    "                                    input_vocab_size,\n",
    "                                    maximum_position_encoding, \n",
    "                                    p)\n",
    "\n",
    "        self.enc_layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.enc_layers.append(EncoderLayer(d_model, \n",
    "                                                num_heads, \n",
    "                                                ff_hidden_dim, \n",
    "                                                p))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) # Transform to (batch_size, input_seq_length, d_model)\n",
    "        # stack multiple to make network \"more powerful\"\n",
    "        # append several encoders together\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torchtext.data as data\n",
    "import torchtext.datasets as datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_len = 200\n",
    "text = data.Field(sequential=True, \n",
    "                  fix_length=max_len, \n",
    "                  batch_first=True, \n",
    "                  lower=True, \n",
    "                  dtype=torch.long)\n",
    "label = data.LabelField(sequential=False, \n",
    "                        dtype=torch.long)\n",
    "\n",
    "# using torch's IMDB dataset https://pytorch.org/text/stable/datasets.html#imdb\n",
    "datasets.IMDB.download('./')\n",
    "ds_train, ds_test = datasets.IMDB.splits(text, label, path='./imdb/aclImdb/')\n",
    "print('train : ', len(ds_train))\n",
    "print('test : ', len(ds_test))\n",
    "print('train.fields :', ds_train.fields)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ds_train, ds_valid = ds_train.split(0.9)\n",
    "print('train : ', len(ds_train))\n",
    "print('valid : ', len(ds_valid))\n",
    "print('test : ', len(ds_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_words = 50000\n",
    "text.build_vocab(ds_train, max_size=num_words)\n",
    "label.build_vocab(ds_train)\n",
    "vocab = text.vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7BFIz_Ef3VH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 164\n",
    "train_loader, valid_loader, test_loader = data.BucketIterator.splits(\n",
    "                                                (ds_train, ds_valid, ds_test), \n",
    "                                                batch_size=batch_size, \n",
    "                                                sort_key=lambda x: len(x.text), \n",
    "                                                repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8wV88T3f3VP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_layers, \n",
    "                 d_model, \n",
    "                 num_heads, \n",
    "                 conv_hidden_dim, \n",
    "                 input_vocab_size, \n",
    "                 num_answers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(num_layers, \n",
    "                               d_model, \n",
    "                               num_heads, \n",
    "                               conv_hidden_dim, \n",
    "                               input_vocab_size,\n",
    "                               maximum_position_encoding=10000)\n",
    "        self.dense = nn.Linear(d_model, num_answers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        x, _ = torch.max(x, dim=1)\n",
    "        x = self.dense(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1DZ7XuTGf3VW",
    "outputId": "9b995f4c-b554-4eed-a2b7-44857c74df3e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerClassifier(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embeddings(\n",
       "      (word_embeddings): Embedding(50002, 32, padding_idx=1)\n",
       "      (position_embeddings): Embedding(10000, 32)\n",
       "      (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (enc_layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (mha): MultiHeadAttention(\n",
       "          (W_q): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (W_k): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (W_v): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (W_h): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (cnn): CNN(\n",
       "          (k1convL1): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (k1convL2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (layernorm1): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dense): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerClassifier(num_layers=1, \n",
    "                              d_model=32, \n",
    "                              num_heads=2, \n",
    "                              conv_hidden_dim=128, \n",
    "                              input_vocab_size=50002, \n",
    "                              num_answers=2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FDXqsS4f3Vb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "epochs = 10\n",
    "t_total = len(train_loader) * epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FlHpWVkzf3Vf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_loader, valid_loader):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_iterator, valid_iterator = iter(train_loader), iter(valid_loader)\n",
    "        nb_batches_train = len(train_loader)\n",
    "        train_acc = 0\n",
    "        model.train()\n",
    "        losses = 0.0\n",
    "\n",
    "        for batch in train_iterator:\n",
    "            x = batch.text.to(device)\n",
    "            y = batch.label.to(device)\n",
    "            \n",
    "            out = model(x)\n",
    "\n",
    "            loss = f.cross_entropy(out, y)\n",
    "            \n",
    "            model.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            losses += loss.item()\n",
    "\n",
    "            optimizer.step()\n",
    "                        \n",
    "            train_acc += (out.argmax(1) == y).cpu().numpy().mean()\n",
    "        \n",
    "        print(\"Training loss at epoch {i} is {:2f}\".format(epoch, \n",
    "                                                           losses / nb_batches_train))\n",
    "        print(\"Training accuracy: {:2%}\".format(train_acc / nb_batches_train))\n",
    "        print('Evaluating on validation:')\n",
    "        evaluate(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBOrlqz3f3Vj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    data_iterator = iter(data_loader)\n",
    "    nb_batches = len(data_loader)\n",
    "    model.eval()\n",
    "    acc = 0 \n",
    "    for batch in data_iterator:\n",
    "        x = batch.text.to(device)\n",
    "        y = batch.label.to(device)\n",
    "                \n",
    "        out = model(x)\n",
    "        acc += (out.argmax(1) == y).cpu().numpy().mean()\n",
    "\n",
    "    print(\"Eval accuracy: {:2%}\".format(acc / nb_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4oNc8LTkf3Vn",
    "outputId": "5884b18e-c6e9-4dd2-950f-fcda42d8e543",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at epoch 0 is 0.7528660064158232\n",
      "Training accuracy: 0.5295102067868505\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.5893673780487805\n",
      "Training loss at epoch 1 is 0.6515302515548208\n",
      "Training accuracy: 0.6335056557087311\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.6597179878048781\n",
      "Training loss at epoch 2 is 0.5937000944994498\n",
      "Training accuracy: 0.7013520678685047\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.6926067073170731\n",
      "Training loss at epoch 3 is 0.49995334036108374\n",
      "Training accuracy: 0.7663706256627787\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.7580411585365855\n",
      "Training loss at epoch 4 is 0.4073252291351125\n",
      "Training accuracy: 0.8173548515376458\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.778887195121951\n",
      "Training loss at epoch 5 is 0.3362871474329976\n",
      "Training accuracy: 0.8570828914810882\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.7899771341463414\n",
      "Training loss at epoch 6 is 0.2818543931496316\n",
      "Training accuracy: 0.8850079533404033\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.8048780487804879\n",
      "Training loss at epoch 7 is 0.23382008626409198\n",
      "Training accuracy: 0.9091938405797102\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.807126524390244\n",
      "Training loss at epoch 8 is 0.19256008691761806\n",
      "Training accuracy: 0.9278897136797454\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.8116615853658536\n",
      "Training loss at epoch 9 is 0.15306968366106352\n",
      "Training accuracy: 0.9477620183810527\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.8085746951219512\n"
     ]
    }
   ],
   "source": [
    "train(train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CwHM0ScJf3Vz",
    "outputId": "20bd8532-ec4e-4e68-cb95-2bf579eb71b0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy: 0.8091711390970119\n"
     ]
    }
   ],
   "source": [
    "evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cDLF9IWe9AOH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}